{
  "name": "Batch_DLP_GCS_Text_to_BigQuery",
  "description": "An example pipeline that reads CSV files from Cloud Storage, uses Cloud DLP API to mask and tokenize data based on the DLP templates provided and stores output in BigQuery. Note, not all configuration settings are available in this default template. You may need to deploy a custom template to accommodate your specific environment and data needs. More details here: https://cloud.google.com/solutions/de-identification-re-identification-pii-using-cloud-dlp",
  "mainClass": "com.google.cloud.teleport.templates.DLPTextToBigQueryBatch",
  "parameters": [
    {
      "name": "inputFilePattern",
      "label": "Input Cloud Storage File(s)",
      "helpText": "The Cloud Storage location of the files you\u0027d like to process. (Example: gs://your-bucket/your-files/*.csv.gz)",
      "isOptional": false,
      "paramType": "TEXT"
    },
    {
      "name": "deidentifyTemplateName",
      "label": "Cloud DLP deidentify template name",
      "helpText": "Cloud DLP template to deidentify contents. Must be created here: https://console.cloud.google.com/security/dlp/create/template. (Example: projects/your-project-id/locations/global/deidentifyTemplates/generated_template_id)",
      "isOptional": false,
      "regexes": [
        "^projects\\/[^\\n\\r\\/]+(\\/locations\\/[^\\n\\r\\/]+)?\\/deidentifyTemplates\\/[^\\n\\r\\/]+$"
      ],
      "paramType": "TEXT"
    },
    {
      "name": "inspectTemplateName",
      "label": "Cloud DLP inspect template name",
      "helpText": "Cloud DLP template to inspect contents. (Example: projects/your-project-id/locations/global/inspectTemplates/generated_template_id)",
      "isOptional": true,
      "regexes": [
        "^projects\\/[^\\n\\r\\/]+(\\/locations\\/[^\\n\\r\\/]+)?\\/inspectTemplates\\/[^\\n\\r\\/]+$"
      ],
      "paramType": "TEXT"
    },
    {
      "name": "batchSize",
      "label": "Batch size",
      "helpText": "Batch size contents (number of rows) to optimize DLP API call. Total size of the rows must not exceed 512 KB and total cell count must not exceed 50,000. Default batch size is set to 100. Ex. 1000",
      "isOptional": true,
      "regexes": [
        "^[0-9]+$"
      ],
      "paramType": "TEXT"
    },
    {
      "name": "dlpProjectId",
      "label": "Cloud DLP project ID",
      "helpText": "Cloud DLP project ID to be used for data masking/tokenization. Ex. your-dlp-project",
      "isOptional": false,
      "regexes": [
        "[a-z0-9\\-\\.\\:]+"
      ],
      "paramType": "TEXT"
    },
    {
      "name": "outputTable",
      "label": "BigQuery output table",
      "helpText": "BigQuery table location to write the output to. The table\u0027s schema must match the input objects.",
      "isOptional": false,
      "regexes": [
        ".+:.+\\..+"
      ],
      "paramType": "TEXT"
    },
    {
      "name": "bigQueryLoadingTemporaryDirectory",
      "label": "Temporary directory for BigQuery loading process",
      "helpText": "Temporary directory for BigQuery loading process (Example: gs://your-bucket/your-files/temp_dir)",
      "isOptional": false,
      "regexes": [
        "^gs:\\/\\/[^\\n\\r]+$"
      ],
      "paramType": "GCS_WRITE_FOLDER"
    },
    {
      "name": "JSONPath",
      "label": "Cloud Storage location of your BigQuery schema file, described as a JSON",
      "helpText": "JSON file with BigQuery Schema description. JSON Example: {\n\t\"BigQuery Schema\": [\n\t\t{\n\t\t\t\"name\": \"location\",\n\t\t\t\"type\": \"STRING\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"name\",\n\t\t\t\"type\": \"STRING\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"age\",\n\t\t\t\"type\": \"STRING\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"color\",\n\t\t\t\"type\": \"STRING\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"coffee\",\n\t\t\t\"type\": \"STRING\"\n\t\t}\n\t]\n}",
      "isOptional": false,
      "regexes": [
        "^gs:\\/\\/[^\\n\\r]+$"
      ],
      "paramType": "GCS_READ_FILE"
    }
  ],
  "runtimeParameters": {}
}